{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(split_ratio=0.5):\n",
    "    # Read in the .csv file and split train test\n",
    "    data_frame = pd.read_csv('Mixcancer.csv')\n",
    "    data = data_frame.to_numpy()\n",
    "\n",
    "    # Strip off the diagnostic column to get the features\n",
    "    x = data[:, 1:]\n",
    "    \n",
    "     # Normalise the dataset\n",
    "    x = MinMaxScaler().fit_transform(x)\n",
    "\n",
    "    # This column becomes the y\n",
    "    y = data[:, 0]\n",
    "    y = y.reshape(len(y), 1).astype(int)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=split_ratio, shuffle=True)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated by Task 4\n",
    "Define activation and loss functions for use in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x)*(1.0-sigmoid(x))\n",
    "\n",
    "def cross_entropy(x, y):\n",
    "    return (-y*(np.log(x)) - (1-y) * np.log(1-x))\n",
    "\n",
    "def cross_entropy_derivative(x, y):\n",
    "    return -(y/x - (1-y)/(1-x))\n",
    "\n",
    "def softmax(x):\n",
    "    max = np.max(x,axis=1,keepdims=True)\n",
    "    exp_raised = np.exp(x - max) # keep softmax stable\n",
    "\n",
    "    # Sum the array along each entry on axis 1\n",
    "    sum_exp = np.sum(exp_raised, axis=1, keepdims=True)\n",
    "    return exp_raised / sum_exp\n",
    "\n",
    "def softmax_derivative(x):\n",
    "    der = softmax(x) * (1-softmax(x))\n",
    "    # der[:, ~y] = -1\n",
    "    return der\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ANN:\n",
    "    def __init__(self, batch_size, learning_rate, epochs):\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def train(self, x_train, y_train, x_test, y_test):\n",
    "        # Initialize weights and bias\n",
    "        neuron_count_l1 = 30\n",
    "        neuron_count_l2 = 2\n",
    "\n",
    "        self.w1 = np.random.uniform(-1, 1, (x_train.shape[1], neuron_count_l1))\n",
    "        self.b1 = np.zeros((1, neuron_count_l1))\n",
    "        self.w2 = np.random.uniform(-1, 1, (neuron_count_l1, neuron_count_l2))\n",
    "        self.b2 = np.zeros((1, neuron_count_l2))\n",
    "\n",
    "        epoch_train_error = []\n",
    "        epoch_train_accuracy = []\n",
    "        epoch_test_error = []\n",
    "        epoch_test_accuracy = []\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            # Mini batches data shuffled on each epoch\n",
    "            random_x_train_indices = np.arange(x_train.shape[0])\n",
    "            np.random.shuffle(random_x_train_indices)\n",
    "\n",
    "            # For each mini batch\n",
    "            mini_batch_train_errors = []\n",
    "            mini_batch_train_accuracies = []\n",
    "\n",
    "            mini_batch_test_errors = []\n",
    "            mini_batch_test_accuracies = []\n",
    "\n",
    "            for i in range(0, random_x_train_indices.shape[0], self.batch_size):\n",
    "                # Test feed forward (confirm network is training correctly)\n",
    "                in_1 = x_test@self.w1 + self.b1\n",
    "                out_1 = sigmoid(in_1)\n",
    "                in_2 = out_1@self.w2 + self.b2\n",
    "                out_2 = softmax(in_2)\n",
    "\n",
    "                test_error = cross_entropy(out_2, y_test).mean()\n",
    "                test_pred = np.argmax(out_2, axis=1)\n",
    "                test_accuracy = accuracy_score(y_test, test_pred)\n",
    "\n",
    "                mini_batch_test_errors.append(test_error)\n",
    "                mini_batch_test_accuracies.append(test_accuracy)\n",
    "\n",
    "                # Train feed forward\n",
    "                x_batch = x_train[i:i + self.batch_size]\n",
    "                y_batch = y_train[i:i + self.batch_size]\n",
    "\n",
    "                in_1 = x_batch@self.w1 + self.b1\n",
    "                out_1 = sigmoid(in_1)\n",
    "                in_2 = out_1@self.w2 + self.b2\n",
    "                out_2 = softmax(in_2)\n",
    "\n",
    "                loss = cross_entropy(out_2, y_batch).mean()\n",
    "\n",
    "                # Find the index of max value of the proability distrubtion (one hot encoding)\n",
    "                train_pred = np.argmax(out_2, axis=1)\n",
    "                train_accuracy = accuracy_score(y_batch, train_pred)\n",
    "\n",
    "                mini_batch_train_errors.append(loss)\n",
    "                mini_batch_train_accuracies.append(train_accuracy)\n",
    "\n",
    "                # Backpropagate using only the training data\n",
    "                # Layer 2\n",
    "\n",
    "                # One hot code y_train\n",
    "                y_hot = np.zeros((len(y_batch), 2))\n",
    "    \n",
    "                # Putting 1 for column where the label is,\n",
    "                # Using multidimensional indexing.\n",
    "                y_hot[np.arange(len(y_batch)), y_batch] = 1\n",
    "    \n",
    "\n",
    "                dE_dO2 = out_2 - y_batch\n",
    "                dO2_dIn2 = softmax_derivative(in_2)\n",
    "                dIn2_dW2 = out_1\n",
    "                dE_dW2 = (1/x_batch.shape[0])*dIn2_dW2.T@(dE_dO2*dO2_dIn2)\n",
    "                dE_dB2 = (1/x_batch.shape[0]) * np.ones([1, len(x_batch)])@(dE_dO2*dO2_dIn2)\n",
    "\n",
    "                # Layer 1\n",
    "                dIn2_dO1 = self.w2\n",
    "                dO1_dIn1 = sigmoid_derivative(in_1)\n",
    "                dIn1_dW1 = x_batch\n",
    "                dE_dW1 = (1/x_batch.shape[0])*dIn1_dW1.T@((dE_dO2@dIn2_dO1.T)*dO1_dIn1)\n",
    "                dE_dB1 = (1/x_batch.shape[0])*np.ones([len(x_batch)])@((dE_dO2@dIn2_dO1.T)*dO1_dIn1)\n",
    "\n",
    "                # Updating parameters\n",
    "                self.b2 -= self.learning_rate * dE_dB2\n",
    "                self.w2 -= self.learning_rate * dE_dW2\n",
    "                self.b1 -= self.learning_rate * dE_dB1\n",
    "                self.w1 -= self.learning_rate * dE_dW1\n",
    "\n",
    "            # Average the error and acuracy across the 2 batches\n",
    "            epoch_train_error.append(np.mean(mini_batch_train_errors))\n",
    "            epoch_train_accuracy.append(np.mean(mini_batch_train_accuracies))\n",
    "\n",
    "            epoch_test_error.append(np.mean(mini_batch_test_errors))\n",
    "            epoch_test_accuracy.append(np.mean(mini_batch_test_accuracies))\n",
    "\n",
    "        # Plot loss for training and test\n",
    "        z = np.arange(self.epochs)\n",
    "        plt.figure(1)\n",
    "        plt.plot(z, epoch_train_error, label=\"train\")\n",
    "        plt.plot(z, epoch_test_error, label=\"test\")\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Cross Entropy Loss')\n",
    "        plt.legend(loc='best')\n",
    "        plt.title('Loss')\n",
    "\n",
    "        # Plot accuracy for training and test\n",
    "        plt.figure(2)\n",
    "        plt.plot(z, epoch_train_accuracy, label=\"train\")\n",
    "        plt.plot(z, epoch_test_accuracy, label=\"test\")\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend(loc='best')\n",
    "        plt.title('Accuracy')\n",
    "\n",
    "    def score(self, x_test, y_test):\n",
    "        in_l1 = x_test@self.w1 + self.b1\n",
    "        out_l1 = sigmoid(in_l1)\n",
    "        in_l2 = out_l1@self.w2 + self.b2\n",
    "        out_l2 = softmax(in_l2)\n",
    "\n",
    "        pred = np.argmax(out_l2, axis=1)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, pred)\n",
    "        average_loss = cross_entropy(out_l2, y_test).mean()\n",
    "        f1 = f1_score(y_test, pred)\n",
    "\n",
    "        print(\"Final Test Accuracy: {:.2f}\".format(accuracy))\n",
    "        print(\"Final Test Loss: {:.2f}\".format(average_loss))\n",
    "        print(\"Final F Measure: {:.2f}\".format(f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    x_train, y_train, x_test, y_test = load_dataset(split_ratio=0.5)\n",
    "\n",
    "    ann = ANN(batch_size=64, learning_rate=0.0015, epochs=10000)\n",
    "\n",
    "    ann.train(x_train, y_train, x_test, y_test)\n",
    "\n",
    "    ann.score(x_test, y_test)\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ab6dd6c404c5d4f755d7817264794fdc58e26a6f703f0f03343b270936447a7d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
