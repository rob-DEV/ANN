{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    # Read in the .csv file and split train test\n",
    "    data_frame = pd.read_csv('Mixcancer.csv')\n",
    "    data = data_frame.to_numpy()\n",
    "\n",
    "    # shuffle the data set\n",
    "    np.random.shuffle(data)\n",
    "    # Strip off the diagnostic column to get the features\n",
    "    x = data[:, 1:]\n",
    "\n",
    "    # Normalise the dataset\n",
    "    x = MinMaxScaler().fit_transform(x)\n",
    "\n",
    "    # This column becomes the y\n",
    "    y = data[:, 0]\n",
    "    y = y.reshape(len(y), 1)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define activation and loss functions for use in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x)*(1.0-sigmoid(x))\n",
    "\n",
    "def cross_entropy(x, y):\n",
    "    return (-y*(np.log(x)) - (1-y) * np.log(1-x))\n",
    "\n",
    "def cross_entropy_derivative(x, y):\n",
    "    return -(y/x - (1-y)/(1-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ANN:\n",
    "    def __init__(self, learning_rate, epochs):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        # Moving here for 10 fold code clarity\n",
    "        # Initialize weights and bias\n",
    "        neuron_count_l1 = 30\n",
    "        neuron_count_l2 = 1\n",
    "\n",
    "        self.w1 = np.random.uniform(-1, 1, (neuron_count_l1, neuron_count_l1))\n",
    "        self.b1 = np.zeros((1, neuron_count_l1))\n",
    "        self.w2 = np.random.uniform(-1, 1, (neuron_count_l1, neuron_count_l2))\n",
    "        self.b2 = np.zeros((1, neuron_count_l2))\n",
    "\n",
    "    def train(self, x, y):\n",
    "        fold_size = int(x.shape[0] / 10) # 25\n",
    "        # Create a fold index\n",
    "        fold_f_measures = []\n",
    "        for fold in range(10):\n",
    "            start_index = fold_size * fold\n",
    "            end_index = start_index + fold_size\n",
    "            fold_indices = range(start_index, end_index, 1)\n",
    "\n",
    "            selection_mask = np.zeros(x.shape[0], dtype=bool)\n",
    "            selection_mask[fold_indices] = True\n",
    "\n",
    "            x_test = x[selection_mask]\n",
    "            y_test = y[selection_mask]\n",
    "\n",
    "            x_train = x[~selection_mask]\n",
    "            y_train = y[~selection_mask]\n",
    "\n",
    "            for epoch_index in range(1, self.epochs+1):\n",
    "\n",
    "                in_1 = x_train@self.w1 + self.b1\n",
    "                out_1 = sigmoid(in_1)\n",
    "                in_2 = out_1@self.w2 + self.b2\n",
    "                out_2 = sigmoid(in_2)\n",
    "\n",
    "                train_error = cross_entropy(out_2, y_train).mean()\n",
    "\n",
    "                # Backpropagate using only the training data\n",
    "                # Layer 2\n",
    "                dE_dO2 = cross_entropy_derivative(out_2, y_train)\n",
    "                dO2_dIn2 = sigmoid_derivative(in_2)\n",
    "                dIn2_dW2 = out_1\n",
    "                dE_dW2 = (1/x_train.shape[0])*dIn2_dW2.T@(dE_dO2*dO2_dIn2)\n",
    "                dE_dB2 = (1/x_train.shape[0]) * np.ones([1, len(x_train)])@(dE_dO2*dO2_dIn2)\n",
    "\n",
    "                # Layer 1\n",
    "                dIn2_dO1 = self.w2\n",
    "                dO1_dIn1 = sigmoid_derivative(in_1)\n",
    "                dIn1_dW1 = x_train\n",
    "                dE_dW1 = (1/x_train.shape[0])*dIn1_dW1.T@((dE_dO2*dO2_dIn2@dIn2_dO1.T)*dO1_dIn1)\n",
    "                dE_dB1 = (1/x_train.shape[0])*np.ones([len(x_train)])@((dE_dO2*dO2_dIn2@dIn2_dO1.T)*dO1_dIn1)\n",
    "\n",
    "                # Updating parameters\n",
    "                self.b2 -= self.learning_rate * dE_dB2\n",
    "                self.w2 -= self.learning_rate * dE_dW2\n",
    "                self.b1 -= self.learning_rate * dE_dB1\n",
    "                self.w1 -= self.learning_rate * dE_dW1\n",
    "\n",
    "                if epoch_index % 100 == 0:\n",
    "                    print(\"Training loss fold {} at epoch: {} = {}\".format(fold + 1, epoch_index, train_error))\n",
    "\n",
    "            # Get F measure of the test set after model training\n",
    "            in_1 = x_test@self.w1 + self.b1\n",
    "            out_1 = sigmoid(in_1)\n",
    "            in_2 = out_1@self.w2 + self.b2\n",
    "            out_2 = sigmoid(in_2)\n",
    "\n",
    "            test_pred = np.where(out_2 > 0.5, 1, 0)\n",
    "            test_f_measure = f1_score(y_test, test_pred)\n",
    "\n",
    "            print(\"F-Measure for fold {} = {}\".format(fold + 1, test_f_measure))\n",
    "            fold_f_measures.append(test_f_measure)\n",
    "\n",
    "            # Reset the weights and baises at the end of each fold's run\n",
    "            self.initialize_weights()\n",
    "        \n",
    "        # Print average F measure score \n",
    "        print(\"F-Measure average for the 10 folds = {}\".format(np.mean(fold_f_measures)))\n",
    "        print(\"F-Measure variance = {}\".format(np.max(fold_f_measures) - np.min(fold_f_measures)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss fold 1 at epoch: 100 = 0.5870528093065274\n",
      "Training loss fold 1 at epoch: 200 = 0.5740949661189264\n",
      "Training loss fold 1 at epoch: 300 = 0.5618221796267864\n",
      "Training loss fold 1 at epoch: 400 = 0.5500280095382002\n",
      "Training loss fold 1 at epoch: 500 = 0.5386744971304253\n",
      "Training loss fold 1 at epoch: 600 = 0.5277278258013873\n",
      "Training loss fold 1 at epoch: 700 = 0.5171577654494934\n",
      "Training loss fold 1 at epoch: 800 = 0.5069376733426735\n",
      "Training loss fold 1 at epoch: 900 = 0.49704435066573976\n",
      "Training loss fold 1 at epoch: 1000 = 0.4874577916461908\n",
      "Training loss fold 1 at epoch: 1100 = 0.47816086481090025\n",
      "Training loss fold 1 at epoch: 1200 = 0.469138959859572\n",
      "Training loss fold 1 at epoch: 1300 = 0.46037962680561767\n",
      "Training loss fold 1 at epoch: 1400 = 0.45187222719624753\n",
      "Training loss fold 1 at epoch: 1500 = 0.44360761090792805\n",
      "Training loss fold 1 at epoch: 1600 = 0.4355778266062056\n",
      "Training loss fold 1 at epoch: 1700 = 0.42777586964869063\n",
      "Training loss fold 1 at epoch: 1800 = 0.42019546800736574\n",
      "Training loss fold 1 at epoch: 1900 = 0.4128309045812757\n",
      "Training loss fold 1 at epoch: 2000 = 0.40567687289205884\n",
      "Training loss fold 1 at epoch: 2100 = 0.3987283624162074\n",
      "Training loss fold 1 at epoch: 2200 = 0.3919805695355135\n",
      "Training loss fold 1 at epoch: 2300 = 0.3854288301333841\n",
      "Training loss fold 1 at epoch: 2400 = 0.37906857011261746\n",
      "Training loss fold 1 at epoch: 2500 = 0.37289527047205195\n",
      "Training loss fold 1 at epoch: 2600 = 0.36690444399247846\n",
      "Training loss fold 1 at epoch: 2700 = 0.36109162100391184\n",
      "Training loss fold 1 at epoch: 2800 = 0.35545234210971527\n",
      "Training loss fold 1 at epoch: 2900 = 0.3499821561125352\n",
      "Training loss fold 1 at epoch: 3000 = 0.3446766217148657\n",
      "F-Measure for fold 1 = 0.927536231884058\n",
      "Training loss fold 2 at epoch: 100 = 0.5226178309034283\n",
      "Training loss fold 2 at epoch: 200 = 0.5111811703506822\n",
      "Training loss fold 2 at epoch: 300 = 0.5006865244154015\n",
      "Training loss fold 2 at epoch: 400 = 0.4904990699952535\n",
      "Training loss fold 2 at epoch: 500 = 0.4806216538595733\n",
      "Training loss fold 2 at epoch: 600 = 0.4710567616260764\n",
      "Training loss fold 2 at epoch: 700 = 0.4618042840727768\n",
      "Training loss fold 2 at epoch: 800 = 0.4528616901371848\n",
      "Training loss fold 2 at epoch: 900 = 0.44422438994203484\n",
      "Training loss fold 2 at epoch: 1000 = 0.4358861624587373\n",
      "Training loss fold 2 at epoch: 1100 = 0.42783956010763113\n",
      "Training loss fold 2 at epoch: 1200 = 0.4200762487511349\n",
      "Training loss fold 2 at epoch: 1300 = 0.4125872757070907\n",
      "Training loss fold 2 at epoch: 1400 = 0.4053632768405855\n",
      "Training loss fold 2 at epoch: 1500 = 0.3983946394621973\n",
      "Training loss fold 2 at epoch: 1600 = 0.39167163576525343\n",
      "Training loss fold 2 at epoch: 1700 = 0.38518453651317486\n",
      "Training loss fold 2 at epoch: 1800 = 0.37892370972159295\n",
      "Training loss fold 2 at epoch: 1900 = 0.3728797056222758\n",
      "Training loss fold 2 at epoch: 2000 = 0.36704332751087815\n",
      "Training loss fold 2 at epoch: 2100 = 0.36140568778980664\n",
      "Training loss fold 2 at epoch: 2200 = 0.3559582490402923\n",
      "Training loss fold 2 at epoch: 2300 = 0.35069285077441037\n",
      "Training loss fold 2 at epoch: 2400 = 0.3456017232771755\n",
      "Training loss fold 2 at epoch: 2500 = 0.3406774904743051\n",
      "Training loss fold 2 at epoch: 2600 = 0.3359131640080572\n",
      "Training loss fold 2 at epoch: 2700 = 0.3313021307056551\n",
      "Training loss fold 2 at epoch: 2800 = 0.3268381354501789\n",
      "Training loss fold 2 at epoch: 2900 = 0.3225152611848067\n",
      "Training loss fold 2 at epoch: 3000 = 0.31832790745904127\n",
      "F-Measure for fold 2 = 0.9180327868852458\n",
      "Training loss fold 3 at epoch: 100 = 0.6642677729846364\n",
      "Training loss fold 3 at epoch: 200 = 0.6500793276218986\n",
      "Training loss fold 3 at epoch: 300 = 0.6383166247930921\n",
      "Training loss fold 3 at epoch: 400 = 0.6267737582420371\n",
      "Training loss fold 3 at epoch: 500 = 0.6154210578646149\n",
      "Training loss fold 3 at epoch: 600 = 0.6042602898427214\n",
      "Training loss fold 3 at epoch: 700 = 0.5932981506626205\n",
      "Training loss fold 3 at epoch: 800 = 0.5825424520595556\n",
      "Training loss fold 3 at epoch: 900 = 0.5719992125533441\n",
      "Training loss fold 3 at epoch: 1000 = 0.5616709491385304\n",
      "Training loss fold 3 at epoch: 1100 = 0.5515562479693296\n",
      "Training loss fold 3 at epoch: 1200 = 0.5416503441097198\n",
      "Training loss fold 3 at epoch: 1300 = 0.531946268925005\n",
      "Training loss fold 3 at epoch: 1400 = 0.5224361371257349\n",
      "Training loss fold 3 at epoch: 1500 = 0.5131122714572223\n",
      "Training loss fold 3 at epoch: 1600 = 0.5039680148295937\n",
      "Training loss fold 3 at epoch: 1700 = 0.4949982009688423\n",
      "Training loss fold 3 at epoch: 1800 = 0.48619932718347103\n",
      "Training loss fold 3 at epoch: 1900 = 0.47756950290774336\n",
      "Training loss fold 3 at epoch: 2000 = 0.4691082502781555\n",
      "Training loss fold 3 at epoch: 2100 = 0.46081622197982547\n",
      "Training loss fold 3 at epoch: 2200 = 0.4526948862649502\n",
      "Training loss fold 3 at epoch: 2300 = 0.4447462143175156\n",
      "Training loss fold 3 at epoch: 2400 = 0.4369723929499846\n",
      "Training loss fold 3 at epoch: 2500 = 0.42937557627672834\n",
      "Training loss fold 3 at epoch: 2600 = 0.4219576831379302\n",
      "Training loss fold 3 at epoch: 2700 = 0.4147202421051933\n",
      "Training loss fold 3 at epoch: 2800 = 0.4076642824114358\n",
      "Training loss fold 3 at epoch: 2900 = 0.40079026676751556\n",
      "Training loss fold 3 at epoch: 3000 = 0.39409806052375745\n",
      "F-Measure for fold 3 = 0.9253731343283583\n",
      "Training loss fold 4 at epoch: 100 = 0.6105192326536053\n",
      "Training loss fold 4 at epoch: 200 = 0.5979217230558941\n",
      "Training loss fold 4 at epoch: 300 = 0.5860821571420393\n",
      "Training loss fold 4 at epoch: 400 = 0.5744184675480503\n",
      "Training loss fold 4 at epoch: 500 = 0.5629301543624837\n",
      "Training loss fold 4 at epoch: 600 = 0.5516246025268601\n",
      "Training loss fold 4 at epoch: 700 = 0.5405127788312181\n",
      "Training loss fold 4 at epoch: 800 = 0.529607288699729\n",
      "Training loss fold 4 at epoch: 900 = 0.5189208934613858\n",
      "Training loss fold 4 at epoch: 1000 = 0.5084654582869192\n",
      "Training loss fold 4 at epoch: 1100 = 0.498251252551126\n",
      "Training loss fold 4 at epoch: 1200 = 0.4882865104783602\n",
      "Training loss fold 4 at epoch: 1300 = 0.4785771845468751\n",
      "Training loss fold 4 at epoch: 1400 = 0.4691268582477353\n",
      "Training loss fold 4 at epoch: 1500 = 0.4599368050274906\n",
      "Training loss fold 4 at epoch: 1600 = 0.4510061800833394\n",
      "Training loss fold 4 at epoch: 1700 = 0.44233231868591805\n",
      "Training loss fold 4 at epoch: 1800 = 0.4339111010268987\n",
      "Training loss fold 4 at epoch: 1900 = 0.42573733783756035\n",
      "Training loss fold 4 at epoch: 2000 = 0.4178051353351935\n",
      "Training loss fold 4 at epoch: 2100 = 0.41010820958328886\n",
      "Training loss fold 4 at epoch: 2200 = 0.40264013443593194\n",
      "Training loss fold 4 at epoch: 2300 = 0.395394519928227\n",
      "Training loss fold 4 at epoch: 2400 = 0.38836512712046567\n",
      "Training loss fold 4 at epoch: 2500 = 0.3815459305540523\n",
      "Training loss fold 4 at epoch: 2600 = 0.37493114125740334\n",
      "Training loss fold 4 at epoch: 2700 = 0.3685152026994421\n",
      "Training loss fold 4 at epoch: 2800 = 0.36229277025509204\n",
      "Training loss fold 4 at epoch: 2900 = 0.35625868241670794\n",
      "Training loss fold 4 at epoch: 3000 = 0.35040792967428264\n",
      "F-Measure for fold 4 = 0.9253731343283582\n",
      "Training loss fold 5 at epoch: 100 = 0.7087883387126004\n",
      "Training loss fold 5 at epoch: 200 = 0.6909773584118203\n",
      "Training loss fold 5 at epoch: 300 = 0.6742164363996735\n",
      "Training loss fold 5 at epoch: 400 = 0.6583039761803123\n",
      "Training loss fold 5 at epoch: 500 = 0.6431366592872026\n",
      "Training loss fold 5 at epoch: 600 = 0.6286288126381075\n",
      "Training loss fold 5 at epoch: 700 = 0.6147099270751641\n",
      "Training loss fold 5 at epoch: 800 = 0.6013223341789149\n",
      "Training loss fold 5 at epoch: 900 = 0.5884190371571986\n",
      "Training loss fold 5 at epoch: 1000 = 0.5759617776061614\n",
      "Training loss fold 5 at epoch: 1100 = 0.5639193711465554\n",
      "Training loss fold 5 at epoch: 1200 = 0.5522663110783358\n",
      "Training loss fold 5 at epoch: 1300 = 0.5409816180499966\n",
      "Training loss fold 5 at epoch: 1400 = 0.5300479027206979\n",
      "Training loss fold 5 at epoch: 1500 = 0.5194506051025218\n",
      "Training loss fold 5 at epoch: 1600 = 0.5091773763755013\n",
      "Training loss fold 5 at epoch: 1700 = 0.4992175742417823\n",
      "Training loss fold 5 at epoch: 1800 = 0.4895618493686152\n",
      "Training loss fold 5 at epoch: 1900 = 0.48020180665658385\n",
      "Training loss fold 5 at epoch: 2000 = 0.4711297300166024\n",
      "Training loss fold 5 at epoch: 2100 = 0.4623383626658417\n",
      "Training loss fold 5 at epoch: 2200 = 0.4538207367371078\n",
      "Training loss fold 5 at epoch: 2300 = 0.44557004661043886\n",
      "Training loss fold 5 at epoch: 2400 = 0.437579560308563\n",
      "Training loss fold 5 at epoch: 2500 = 0.429842563007249\n",
      "Training loss fold 5 at epoch: 2600 = 0.4223523265323279\n",
      "Training loss fold 5 at epoch: 2700 = 0.4151020988264025\n",
      "Training loss fold 5 at epoch: 2800 = 0.4080851078085478\n",
      "Training loss fold 5 at epoch: 2900 = 0.40129457475994673\n",
      "Training loss fold 5 at epoch: 3000 = 0.39472373323880133\n",
      "F-Measure for fold 5 = 0.9375\n",
      "Training loss fold 6 at epoch: 100 = 0.6779414516951567\n",
      "Training loss fold 6 at epoch: 200 = 0.660926697855802\n",
      "Training loss fold 6 at epoch: 300 = 0.6475353070224127\n",
      "Training loss fold 6 at epoch: 400 = 0.6347496076415923\n",
      "Training loss fold 6 at epoch: 500 = 0.6224527936164607\n",
      "Training loss fold 6 at epoch: 600 = 0.6105590446055043\n",
      "Training loss fold 6 at epoch: 700 = 0.5990018086769211\n",
      "Training loss fold 6 at epoch: 800 = 0.5877308959377142\n",
      "Training loss fold 6 at epoch: 900 = 0.5767097770099483\n",
      "Training loss fold 6 at epoch: 1000 = 0.565913104417651\n",
      "Training loss fold 6 at epoch: 1100 = 0.5553244681801247\n",
      "Training loss fold 6 at epoch: 1200 = 0.5449343908680251\n",
      "Training loss fold 6 at epoch: 1300 = 0.5347385688673116\n",
      "Training loss fold 6 at epoch: 1400 = 0.5247363664372855\n",
      "Training loss fold 6 at epoch: 1500 = 0.5149295643759679\n",
      "Training loss fold 6 at epoch: 1600 = 0.505321356509365\n",
      "Training loss fold 6 at epoch: 1700 = 0.49591557724967994\n",
      "Training loss fold 6 at epoch: 1800 = 0.48671613467030544\n",
      "Training loss fold 6 at epoch: 1900 = 0.47772661766463065\n",
      "Training loss fold 6 at epoch: 2000 = 0.4689500434445517\n",
      "Training loss fold 6 at epoch: 2100 = 0.46038871262043174\n",
      "Training loss fold 6 at epoch: 2200 = 0.4520441425546829\n",
      "Training loss fold 6 at epoch: 2300 = 0.4439170545727148\n",
      "Training loss fold 6 at epoch: 2400 = 0.43600739599857674\n",
      "Training loss fold 6 at epoch: 2500 = 0.4283143831244273\n",
      "Training loss fold 6 at epoch: 2600 = 0.42083655564816463\n",
      "Training loss fold 6 at epoch: 2700 = 0.4135718365898942\n",
      "Training loss fold 6 at epoch: 2800 = 0.4065175941892927\n",
      "Training loss fold 6 at epoch: 2900 = 0.39967070389441955\n",
      "Training loss fold 6 at epoch: 3000 = 0.3930276094597618\n",
      "F-Measure for fold 6 = 0.9253731343283583\n",
      "Training loss fold 7 at epoch: 100 = 0.7056603027225868\n",
      "Training loss fold 7 at epoch: 200 = 0.6710071556556785\n",
      "Training loss fold 7 at epoch: 300 = 0.6563247099198446\n",
      "Training loss fold 7 at epoch: 400 = 0.6426016750813205\n",
      "Training loss fold 7 at epoch: 500 = 0.6294372917032565\n",
      "Training loss fold 7 at epoch: 600 = 0.6167693943655322\n",
      "Training loss fold 7 at epoch: 700 = 0.6045479264911808\n",
      "Training loss fold 7 at epoch: 800 = 0.5927288828703808\n",
      "Training loss fold 7 at epoch: 900 = 0.5812736347518225\n",
      "Training loss fold 7 at epoch: 1000 = 0.5701484910919822\n",
      "Training loss fold 7 at epoch: 1100 = 0.5593243453919405\n",
      "Training loss fold 7 at epoch: 1200 = 0.5487763597313406\n",
      "Training loss fold 7 at epoch: 1300 = 0.5384836574849232\n",
      "Training loss fold 7 at epoch: 1400 = 0.5284290100773038\n",
      "Training loss fold 7 at epoch: 1500 = 0.5185985124115391\n",
      "Training loss fold 7 at epoch: 1600 = 0.5089812471488009\n",
      "Training loss fold 7 at epoch: 1700 = 0.4995689409099911\n",
      "Training loss fold 7 at epoch: 1800 = 0.49035561674293593\n",
      "Training loss fold 7 at epoch: 1900 = 0.48133724764228414\n",
      "Training loss fold 7 at epoch: 2000 = 0.47251141602532054\n",
      "Training loss fold 7 at epoch: 2100 = 0.46387698409542616\n",
      "Training loss fold 7 at epoch: 2200 = 0.4554337800144856\n",
      "Training loss fold 7 at epoch: 2300 = 0.44718230469799\n",
      "Training loss fold 7 at epoch: 2400 = 0.43912346375317957\n",
      "Training loss fold 7 at epoch: 2500 = 0.4312583285362582\n",
      "Training loss fold 7 at epoch: 2600 = 0.42358792949482854\n",
      "Training loss fold 7 at epoch: 2700 = 0.41611308392401913\n",
      "Training loss fold 7 at epoch: 2800 = 0.4088342590752471\n",
      "Training loss fold 7 at epoch: 2900 = 0.40175147030962155\n",
      "Training loss fold 7 at epoch: 3000 = 0.39486421277908323\n",
      "F-Measure for fold 7 = 0.9354838709677419\n",
      "Training loss fold 8 at epoch: 100 = 0.6648160923032289\n",
      "Training loss fold 8 at epoch: 200 = 0.6521741044364966\n",
      "Training loss fold 8 at epoch: 300 = 0.6402264205183203\n",
      "Training loss fold 8 at epoch: 400 = 0.6284867739803979\n",
      "Training loss fold 8 at epoch: 500 = 0.6169273721838686\n",
      "Training loss fold 8 at epoch: 600 = 0.6055369449227854\n",
      "Training loss fold 8 at epoch: 700 = 0.5943138345080201\n",
      "Training loss fold 8 at epoch: 800 = 0.5832626929218784\n",
      "Training loss fold 8 at epoch: 900 = 0.5723913282411794\n",
      "Training loss fold 8 at epoch: 1000 = 0.5617081666924391\n",
      "Training loss fold 8 at epoch: 1100 = 0.5512206023922318\n",
      "Training loss fold 8 at epoch: 1200 = 0.5409342098821082\n",
      "Training loss fold 8 at epoch: 1300 = 0.5308525898801683\n",
      "Training loss fold 8 at epoch: 1400 = 0.5209775644855943\n",
      "Training loss fold 8 at epoch: 1500 = 0.5113094983687272\n",
      "Training loss fold 8 at epoch: 1600 = 0.5018476229563608\n",
      "Training loss fold 8 at epoch: 1700 = 0.4925903227693677\n",
      "Training loss fold 8 at epoch: 1800 = 0.48353538512137506\n",
      "Training loss fold 8 at epoch: 1900 = 0.4746802231733613\n",
      "Training loss fold 8 at epoch: 2000 = 0.46602207523399314\n",
      "Training loss fold 8 at epoch: 2100 = 0.4575581746430366\n",
      "Training loss fold 8 at epoch: 2200 = 0.4492858813121159\n",
      "Training loss fold 8 at epoch: 2300 = 0.44120276839575506\n",
      "Training loss fold 8 at epoch: 2400 = 0.43330666301383125\n",
      "Training loss fold 8 at epoch: 2500 = 0.4255956455914697\n",
      "Training loss fold 8 at epoch: 2600 = 0.4180680164267499\n",
      "Training loss fold 8 at epoch: 2700 = 0.41072223995487384\n",
      "Training loss fold 8 at epoch: 2800 = 0.4035568770963191\n",
      "Training loss fold 8 at epoch: 2900 = 0.3965705146734351\n",
      "Training loss fold 8 at epoch: 3000 = 0.3897616988095545\n",
      "F-Measure for fold 8 = 0.888888888888889\n",
      "Training loss fold 9 at epoch: 100 = 0.6776881085377516\n",
      "Training loss fold 9 at epoch: 200 = 0.6610832355287125\n",
      "Training loss fold 9 at epoch: 300 = 0.6505987815059429\n",
      "Training loss fold 9 at epoch: 400 = 0.6405782779918504\n",
      "Training loss fold 9 at epoch: 500 = 0.6308738890007598\n",
      "Training loss fold 9 at epoch: 600 = 0.6214321731230327\n",
      "Training loss fold 9 at epoch: 700 = 0.612209137791517\n",
      "Training loss fold 9 at epoch: 800 = 0.6031679314271498\n",
      "Training loss fold 9 at epoch: 900 = 0.5942779291914102\n",
      "Training loss fold 9 at epoch: 1000 = 0.5855139758968694\n",
      "Training loss fold 9 at epoch: 1100 = 0.5768557435823854\n",
      "Training loss fold 9 at epoch: 1200 = 0.5682871825691247\n",
      "Training loss fold 9 at epoch: 1300 = 0.5597960482768743\n",
      "Training loss fold 9 at epoch: 1400 = 0.5513734890217351\n",
      "Training loss fold 9 at epoch: 1500 = 0.5430136823836805\n",
      "Training loss fold 9 at epoch: 1600 = 0.5347135093604017\n",
      "Training loss fold 9 at epoch: 1700 = 0.5264722564373391\n",
      "Training loss fold 9 at epoch: 1800 = 0.5182913361123223\n",
      "Training loss fold 9 at epoch: 1900 = 0.5101740166878102\n",
      "Training loss fold 9 at epoch: 2000 = 0.5021251527607578\n",
      "Training loss fold 9 at epoch: 2100 = 0.49415090927742766\n",
      "Training loss fold 9 at epoch: 2200 = 0.48625847461099425\n",
      "Training loss fold 9 at epoch: 2300 = 0.4784557618984973\n",
      "Training loss fold 9 at epoch: 2400 = 0.4707511024756479\n",
      "Training loss fold 9 at epoch: 2500 = 0.46315293992053397\n",
      "Training loss fold 9 at epoch: 2600 = 0.45566953697904194\n",
      "Training loss fold 9 at epoch: 2700 = 0.4483087095644628\n",
      "Training loss fold 9 at epoch: 2800 = 0.4410776015275888\n",
      "Training loss fold 9 at epoch: 2900 = 0.4339825109785535\n",
      "Training loss fold 9 at epoch: 3000 = 0.42702877419961255\n",
      "F-Measure for fold 9 = 0.8955223880597014\n",
      "Training loss fold 10 at epoch: 100 = 0.6452535443021367\n",
      "Training loss fold 10 at epoch: 200 = 0.6326077259603695\n",
      "Training loss fold 10 at epoch: 300 = 0.6212880263988952\n",
      "Training loss fold 10 at epoch: 400 = 0.6102877444250091\n",
      "Training loss fold 10 at epoch: 500 = 0.5995501050482114\n",
      "Training loss fold 10 at epoch: 600 = 0.5890366479097221\n",
      "Training loss fold 10 at epoch: 700 = 0.5787174520632365\n",
      "Training loss fold 10 at epoch: 800 = 0.5685696797587281\n",
      "Training loss fold 10 at epoch: 900 = 0.5585764197601681\n",
      "Training loss fold 10 at epoch: 1000 = 0.5487257444376292\n",
      "Training loss fold 10 at epoch: 1100 = 0.5390099339003813\n",
      "Training loss fold 10 at epoch: 1200 = 0.5294248196409503\n",
      "Training loss fold 10 at epoch: 1300 = 0.5199692053456018\n",
      "Training loss fold 10 at epoch: 1400 = 0.5106443312784729\n",
      "Training loss fold 10 at epoch: 1500 = 0.5014533599217482\n",
      "Training loss fold 10 at epoch: 1600 = 0.4924008730890484\n",
      "Training loss fold 10 at epoch: 1700 = 0.4834923828263076\n",
      "Training loss fold 10 at epoch: 1800 = 0.4747338681678655\n",
      "Training loss fold 10 at epoch: 1900 = 0.4661313555685078\n",
      "Training loss fold 10 at epoch: 2000 = 0.45769056169129557\n",
      "Training loss fold 10 at epoch: 2100 = 0.4494166134074711\n",
      "Training loss fold 10 at epoch: 2200 = 0.4413138526594594\n",
      "Training loss fold 10 at epoch: 2300 = 0.43338572524399865\n",
      "Training loss fold 10 at epoch: 2400 = 0.4256347446549753\n",
      "Training loss fold 10 at epoch: 2500 = 0.4180625164403682\n",
      "Training loss fold 10 at epoch: 2600 = 0.4106698057948323\n",
      "Training loss fold 10 at epoch: 2700 = 0.4034566312142479\n",
      "Training loss fold 10 at epoch: 2800 = 0.396422369281614\n",
      "Training loss fold 10 at epoch: 2900 = 0.3895658590951435\n",
      "Training loss fold 10 at epoch: 3000 = 0.3828854986067644\n",
      "F-Measure for fold 10 = 0.923076923076923\n",
      "F-Measure average for the 10 folds = 0.9202160492747634\n",
      "F-Measure variance = 0.04861111111111105\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # load the dataset with applied random shuffle\n",
    "    x, y = load_dataset()\n",
    "\n",
    "    ann = ANN(learning_rate=0.008, epochs=3000)\n",
    "\n",
    "    ann.train(x, y)\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ab6dd6c404c5d4f755d7817264794fdc58e26a6f703f0f03343b270936447a7d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
